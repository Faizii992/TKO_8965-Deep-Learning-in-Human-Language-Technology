
BERT based text classifier

Here is a notebook for training a BERT based text classifier. This model loads a pre-trained BERT encoder and fine-tunes it for text classification (learns a classification layer on top of the encoder).

a) Study the notebook to see how the model is created. Summarize the model creation part (note that most of the code we have already seen previously in CNN/RNN notebooks, so no need to summarize these parts anymore). Especially, you should explain what AutoModel and ForSequenceClassification means in transformers.AutoModelForSequenceClassification.from_pretrained().

b) How the accuracy compares with your previous CNN/RNN experiments?
